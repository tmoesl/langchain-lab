"""
LangGraph - Tools and ToolNode

Demonstrates how to augment an LLM with tools and use the prebuilt ToolNode
to execute tool calls. This pattern enables agents to perform actions and
computation using external functions.

Key Concepts:
-------------
- Tool Binding: Connecting tools to the LLM so it knows their schemas
- ToolNode: Prebuilt node that executes tool calls generated by the LLM
- Conditional Edges: Routing based on whether the LLM requested a tool call
- Message History: Maintaining the conversation context including tool results

Two Tool Implementation Options:
--------------------------------
1. Prebuilt ToolNode (Standard):
   - from langgraph.prebuilt import ToolNode
   - Best for production; handles errors, parallel execution, and state updates automatically.

2. Custom Tool Node (Educational):
   - Manual implementation showing the "under the hood" logic.
   - Iterates over tool calls, invokes functions, and constructs ToolMessages.
   - Useful for learning or when you need highly custom execution logic (e.g., human approval).

Toggle between these by uncommenting the desired line in the "Build Graph" section.

Execution Flow:
---------------
1. Agent node invokes LLM with current history
2. Router checks if LLM produced a tool call
   - If YES -> route to "tools" node
   - If NO  -> route to END (respond to user)
3. Tools node executes the requested tool(s) and adds results to state
4. Loop back to Agent node to process tool results

References:
https://docs.langchain.com/oss/python/langgraph/quickstart
https://docs.langchain.com/oss/python/langchain/overview
"""

from typing import Annotated, Literal, TypedDict

from dotenv import load_dotenv
from IPython.display import Image, display
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages

load_dotenv()


# ==============================================================
# Define State
# ==============================================================


class AgentState(TypedDict):
    """State of the agent with message history and counter for model calls."""

    messages: Annotated[list[BaseMessage], add_messages]
    counter_model_calls: int


# ==============================================================
# Define Tools
# ==============================================================
# @tool decorator to define tools (instance of StructuredTool)


@tool(parse_docstring=True)
def multiply(a: float, b: float) -> float:
    """Multiply two numbers.

    Args:
        a (float): First number
        b (float): Second number

    Returns:
        float: The product of a and b
    """
    return a * b


@tool(parse_docstring=True)
def add(a: float, b: float) -> float:
    """Add two numbers.

    Args:
        a (float): First number
        b (float): Second number

    Returns:
        float: The sum of a and b
    """
    return a + b


@tool(parse_docstring=True)
def divide(a: float, b: float) -> float:
    """Divide two numbers.

    Args:
        a (float): First number
        b (float): Second number

    Returns:
        float: The quotient of a divided by b

    Raises:
        ValueError: If b is zero
    """
    if b == 0:
        raise ValueError("Cannot divide by zero.")
    return a / b


# List of tools available to the agent
tools = [add, multiply, divide]
tools_by_name = {t.name: t for t in tools}


# ==============================================================
# Initialize Model
# ==============================================================

model = ChatOpenAI(model="gpt-5-mini")
model_with_tools = model.bind_tools(tools)


# ==============================================================
# Define Nodes
# ==============================================================


def agent_node(state: AgentState) -> dict:
    """
    Invokes the model with the current state.
    The model will decide whether to call a tool or respond directly.
    Updates the counter for model calls.
    """
    print("\n--- Agent Node ---")

    system_msg = SystemMessage(
        content="""You are a helpful assistant that performs arithmetic operations.
        For tasks with dependencies, call tools sequentially. Wait for each result before
        using it in the next tool call. Plan your actions and call the tools accordingly.
        Always return the answer in simple language."""
    )

    messages = [system_msg] + state["messages"]
    response = model_with_tools.invoke(messages)
    counter = state.get("counter_model_calls", 0) + 1

    return {"messages": [response], "counter_model_calls": counter}


# NOTE: We typically use the prebuilt ToolNode, but here is an example of what
# it does under the hood. You can use this for custom behavior.
def tool_node(state: AgentState) -> dict:
    """
    Executes tool calls manually.
    1. Iterates over tool calls in the last message
    2. Executes the corresponding tool
    3. Returns a ToolMessage with the result and matching tool_call_id
    """
    last_message = state["messages"][-1]
    tool_results = []

    for tool_call in last_message.tool_calls:  # type: ignore
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        tool_id = tool_call["id"]

        print(f"  - Executing Tool: {tool_name} with args: {tool_args}")

        # Execute tool
        tool_instance = tools_by_name[tool_name]
        output = tool_instance.invoke(tool_args)

        # Create ToolMessage (CRITICAL: must match tool_call_id)
        tool_results.append(ToolMessage(content=str(output), tool_call_id=tool_id))

    return {"messages": tool_results}


# ==============================================================
# Routing Functions
# ==============================================================


def should_continue(state: AgentState) -> Literal["tools", END]:  # type: ignore
    """
    Determines the next step based on the last message.
    - If tool calls are present -> route to 'tools'
    - Otherwise -> END
    """
    last_message = state["messages"][-1]

    if last_message.tool_calls:  # type: ignore
        print("  - Routing to: tools")
        return "tools"

    print("  - Routing to: END")
    return END


# ==============================================================
# Build Graph
# ==============================================================

# Initialize graph
graph = StateGraph(AgentState)

# Add nodes
graph.add_node("agent", agent_node)
# graph.add_node("tools", ToolNode(tools))  # Prebuilt (Standard)
graph.add_node("tools", tool_node)  # Custom (Educational)

# Add static and conditional edges
graph.add_edge(START, "agent")
graph.add_conditional_edges("agent", should_continue, ["tools", END])
graph.add_edge("tools", "agent")

# Compile graph
graph = graph.compile()

# ==============================================================
# Run Demonstration
# ==============================================================

print("=" * 70)
print("LangGraph Tools Demonstration")
print("=" * 70)

# Invoke graph
message = [HumanMessage(content="Multiply 2 by 5 and then add 10 to the result.")]
response = graph.invoke({"messages": message}, config={"recursion_limit": 10})  # type: ignore
print(f"\nUser: {message[0].content}")
print(f"\nAssistant: {response['messages'][-1].content}")

# Debug agent (observe all messages)
print("\n[Execution Trace]")
for msg in response["messages"]:
    msg.pretty_print()

# ==============================================================
# Visualize Graph
# ==============================================================
print("\n" + "=" * 60)
print("Graph Visualization:")
print("=" * 60)

display(Image(graph.get_graph().draw_mermaid_png()))
