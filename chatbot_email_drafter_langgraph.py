from typing import Annotated, TypedDict

from dotenv import load_dotenv
from IPython.display import Image, display
from langchain.tools import ToolRuntime, tool
from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.messages.utils import count_tokens_approximately, trim_messages
from langchain_core.runnables.config import RunnableConfig
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.types import Command
from pydantic import BaseModel, Field

load_dotenv()

model = ChatOpenAI(model="gpt-4o-mini")


class Context(BaseModel):
    """Runtime context for the agent."""

    user_id: str = Field(default="user_id", description="The user ID for the conversation")


class AgentState(TypedDict):
    """List of messages and document content"""

    messages: Annotated[list[AnyMessage], add_messages]
    document_content: str


# Command needs to include a ToolMessage. Not autogenerated
@tool
def update(text: str, runtime: ToolRuntime[Context]) -> Command:
    """Update the document content with the new content

    Args:
        text: The text to update the document content with
    """

    updated_content = f"UserId: {runtime.context.user_id}\n\n{text}"

    return Command(
        update={
            "document_content": updated_content,
            "messages": [
                ToolMessage(content="Document updated", tool_call_id=runtime.tool_call_id)
            ],
        }
    )


# ToolMessage automatically added by the ToolNode
# Note: Context is injected into runtime of ALL tools at invoke time, even if not used.
# Type annotation must match actual data for proper serialization by checkpointer.
@tool
def save(filename: str, runtime: ToolRuntime[Context]) -> str:
    """Save the document content to a file

    Args:
        filename: The name of the file to save the document content to
    """

    content = runtime.state.get("document_content", "")

    if not content:
        return "No document content found."

    if not filename.endswith(".txt"):
        filename += ".txt"

    try:
        with open(filename, "w") as f:
            f.write(content)
        return f"Document saved to {filename}"
    except Exception as e:
        return f"Error saving document: {e}"


# Augment the LLM with tools
# tools: list of instances of StructuredTool (via @tool decorator)
tools = [update, save]
model_with_tools = model.bind_tools(tools)


def agent_node(state: AgentState) -> dict:
    """Calls LLM with tools, returns response and updates the LLM call count"""

    system_prompt = """You are a helpful assistant that updates the document content. 
        Extract the text that should be used to update the document.
        Use the tools to update the document content or save the document content to a file."""

    current_content = state.get("document_content", "")

    if current_content:
        system_msg = f"{system_prompt}\n\n Current document: {current_content}"
        print(f"\n\n{current_content}\n\n")
    else:
        system_msg = system_prompt

    # Token-based trimming
    # count_tokens_approximately handles message objects
    # model.get_num_tokens expects plain strings only
    trimmed_messages = trim_messages(
        messages=state["messages"],
        strategy="last",  # Keep most recent messages
        token_counter=count_tokens_approximately,
        max_tokens=500,
        start_on="human",
        end_on=("human", "tool"),
        include_system=False,
    )

    messages = [SystemMessage(content=system_msg)] + trimmed_messages
    response = model_with_tools.invoke(messages)
    return {"messages": [response]}


def should_continue(state: AgentState) -> str:
    """Routes to tools if the last message has tool calls, otherwise ends."""
    last_message = state["messages"][-1]

    return "tools" if last_message.tool_calls else END


# Compile with checkpointer
memory = InMemorySaver()
graph = StateGraph(AgentState)

graph.add_node("agent", agent_node)
graph.add_node("tools", ToolNode(tools))

graph.add_edge(START, "agent")
graph.add_conditional_edges("agent", should_continue, ["tools", END])
graph.add_edge("tools", "agent")

agent = graph.compile(checkpointer=memory)

# Visualize the graph
display(Image(agent.get_graph().draw_mermaid_png()))

# Thread ID tracks conversation
config: RunnableConfig = {"configurable": {"thread_id": "1"}}  # type: ignore

while True:
    user_input = input("Enter: ")
    if user_input.lower() == "exit":
        break

    message = [HumanMessage(content=user_input)]
    response = agent.invoke({"messages": message}, context=Context(user_id="123"), config=config)  # type: ignore
    print(f"Assistant: {response['messages'][-1].content}\n")
